{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLGf0zIrEXvTCeyn1wjMqd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mouneeshsaravanan/BHARATH-INTERN-/blob/main/Project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE5B0-Eu1l_d",
        "outputId": "f56da6a0-8b1c-415e-e0cc-dfb187a3fa58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "140/140 [==============================] - 4s 24ms/step - loss: 0.3451 - accuracy: 0.8766 - val_loss: 0.0985 - val_accuracy: 0.9812\n",
            "Epoch 2/5\n",
            "140/140 [==============================] - 3s 22ms/step - loss: 0.0508 - accuracy: 0.9861 - val_loss: 0.0487 - val_accuracy: 0.9874\n",
            "Epoch 3/5\n",
            "140/140 [==============================] - 4s 28ms/step - loss: 0.0145 - accuracy: 0.9964 - val_loss: 0.0487 - val_accuracy: 0.9883\n",
            "Epoch 4/5\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 0.0053 - accuracy: 0.9991 - val_loss: 0.0538 - val_accuracy: 0.9892\n",
            "Epoch 5/5\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0564 - val_accuracy: 0.9874\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0564 - accuracy: 0.9874\n",
            "External Dataset Model Evaluation:\n",
            "Accuracy: 0.9874439239501953\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "\n",
            "Predictions for New Dataset:\n",
            "                              text predicted_label\n",
            "0        Claim your prize now!                spam\n",
            "1   Meeting rescheduled to 3 PM.          not spam\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load external dataset for training and testing with explicit encoding\n",
        "external_data_path = '/content/spam.csv'\n",
        "external_data = pd.read_csv(external_data_path, encoding='latin-1')\n",
        "\n",
        "# Text preprocessing for external dataset\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    text = text.lower().split()\n",
        "    text = [ps.stem(word) for word in text if not word in stop_words]\n",
        "    text = ' '.join(text)\n",
        "    return text\n",
        "\n",
        "# Convert string labels to integers\n",
        "external_data['label'] = (external_data['label'] == 'spam').astype(int)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(external_data['text'])\n",
        "X = tokenizer.texts_to_sequences(external_data['text'])\n",
        "\n",
        "# Padding sequences\n",
        "maxlen = 100  # maximum sequence length\n",
        "X = pad_sequences(X, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Define CNN model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=50, input_length=maxlen),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split external data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, external_data['label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on the external testing set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"External Dataset Model Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Load the new dataset for prediction\n",
        "new_dataset_path = '/content/prediction spam msg.csv'\n",
        "new_dataset = pd.read_csv(new_dataset_path)\n",
        "\n",
        "# Text preprocessing for the new dataset for prediction\n",
        "new_dataset['processed_text'] = new_dataset['text'].apply(preprocess_text)\n",
        "X_new = tokenizer.texts_to_sequences(new_dataset['processed_text'])\n",
        "X_new = pad_sequences(X_new, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Define a function to convert label from 1/0 to \"spam\"/\"not spam\"\n",
        "def label_to_text(label):\n",
        "    return \"spam\" if label == 1 else \"not spam\"\n",
        "\n",
        "# Predict using the trained model\n",
        "predictions = model.predict(X_new)\n",
        "predictions_binary = (predictions > 0.5).astype(int)\n",
        "predictions_text = [label_to_text(pred) for pred in predictions_binary]\n",
        "\n",
        "# Display predictions for the new dataset\n",
        "new_dataset['predicted_label'] = predictions_text\n",
        "print(\"\\nPredictions for New Dataset:\")\n",
        "print(new_dataset[['text', 'predicted_label']])\n"
      ]
    }
  ]
}